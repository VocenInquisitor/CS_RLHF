# -*- coding: utf-8 -*-
"""InferenceTesting.ipynb

Automatically generated by Colab.

"""

# --------------------------------------------
#1. Imports and Environment Setup
# --------------------------------------------
import os
from huggingface_hub import login

# Set Hugging Face authentication token
HF_TOKEN = "hf_token"  # Replace if needed

# allow training to use 4 GPUs
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

# Redirect Hugging Face cache to project directory
HF_CACHE_DIR = "store cache"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR
os.environ["HF_DATASETS_CACHE"] = HF_CACHE_DIR


# Optional: disable W&B logging (or set your API key if enabled)
os.environ["WANDB_MODE"] = "disabled"
# os.environ["WANDB_API_KEY"] = "your_wandb_api_key"

# Login to Hugging Face
login(HF_TOKEN)
print("Hugging Face authenticated and environment set.")

# Path to your trained PPO actor model (policy)
model_path = "path to model the model"

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load model from sharded files
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",  # Automatically maps across GPUs
    torch_dtype=torch.float16,  # if trained in bf16 use torch.bfloat16
)

model.eval()
print("PPO policy model loaded successfully!")

# Path to the prompt file you uploaded
prompt_file = "500Prompts.txt"

with open(prompt_file, "r") as f:
    prompts = [line.strip() for line in f if line.strip()]

print(f" Loaded {len(prompts)} prompts.")

responses = []

for idx, prompt in enumerate(prompts):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=512,
            # do_sample=True,
            # temperature=0.7,
            # top_k=50,
            # top_p=0.95,
            #repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id
        )
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    responses.append((prompt, decoded))
    print(f"\n Prompt {idx + 1}: {prompt}\n Response: {decoded}\n")

######################## Generate 10 responses per prompt#########################################

# responses = []
# for idx, prompt in enumerate(prompts):
#     print(f"\n Prompt {idx + 1}: {prompt}")
#     for i in range(10):
#         inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#         with torch.no_grad():
#             output = model.generate(
#                 **inputs,
#                 max_new_tokens=512,
#                 do_sample=True,             # Enable sampling for diversity
#                 #temperature=0.7,
#                 #top_k=50,
#                 #top_p=0.95,
#                 #repetition_penalty=1.2,
#                 pad_token_id=tokenizer.eos_token_id
#             )
#         decoded = tokenizer.decode(output[0], skip_special_tokens=True)
#         responses.append((prompt, f"Response {i+1}:\n{decoded.strip()}"))
#         #print(f" Response {i+1}: {decoded.strip()[:100]}...")

output_path = "./saferesults"

with open(output_path, "w") as f:
    for prompt, response in responses:
        f.write(f"PROMPT:\n{prompt}\n\nRESPONSE:\n{response}\n\n{'='*80}\n")

print(f"All responses saved to: {output_path}")

"""## Multi_turn Testing"""

# Path to user-only prompts (one user input per line)
prompt_file = "./PromptConverstion.txt"

with open(prompt_file, "r") as f:
    user_turns = [line.strip() for line in f if line.strip()]
print(f"Loaded {len(user_turns)} user prompts.\n")

# Helper to extract assistant reply
def extract_reply(decoded: str) -> str:
    return decoded.rsplit("[/INST]", 1)[-1].split("</s>")[0].strip()

# Multi-turn processing
history = ""
dialogue = []
MAX_NEW_TOKENS = 256
CTX_MAX = 4096
TRUNCATE_TO_LAST_N = 3000
SAFETY_MARGIN = 256

for idx, user_msg in enumerate(user_turns, start=1):
    history += f"<s>[INST] {user_msg} [/INST]"

    token_count = len(tokenizer(history)["input_ids"])
    if token_count + MAX_NEW_TOKENS + SAFETY_MARGIN > CTX_MAX:
        trimmed = tokenizer(history)["input_ids"][-TRUNCATE_TO_LAST_N:]
        history = tokenizer.decode(trimmed, skip_special_tokens=False)
        print(f" Truncated history to last {TRUNCATE_TO_LAST_N} tokens.")

    inputs = tokenizer(history, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    assistant = extract_reply(decoded)

    print(f"\n User [{idx}]: {user_msg}")
    print(f" Bot  [{idx}]: {assistant}\n")

    dialogue.append((user_msg, assistant))
    history += f" {assistant} </s>\n"

# Save full conversation
with open("ReluMultiturn_conversation.txt", "w") as f:
    for u, a in dialogue:
        f.write(f"USER: {u}\nASSISTANT: {a}\n\n")
print("All responses saved to: conversation_log.txt")
