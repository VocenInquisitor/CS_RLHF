# -*- coding: utf-8 -*-
"""TrainPolicyModel.ipynb

Automatically generated by Colab.
"""

import os, json, shutil, pprint, torch
from transformers import AutoTokenizer
from safe_rlhf.models import AutoModelForScore

# 1)  Path to your checkpoint
MODEL_DIR = "Path to model"
CFG_PATH  = os.path.join(MODEL_DIR, "config.json")

# 2)  Back-up the original once
if not os.path.exists(CFG_PATH + ".bak"):
    shutil.copy2(CFG_PATH, CFG_PATH + ".bak")
    print(f"Backup saved ➜ {CFG_PATH}.bak")

# 3)  Patch the Safe-RLHF keys
with open(CFG_PATH) as f:
    cfg = json.load(f)

cfg["architectures"] = ["LlamaForScore"]   # wrapper class Safe-RLHF expects
cfg["score_dim"]     = 1                   # single scalar
cfg["score_type"]    = "cost"              # interpret as harmfulness score
cfg.pop("auto_map", None)                  # ← remove auto_map completely
cfg["model_type"]   = "llama"

with open(CFG_PATH, "w") as f:
    json.dump(cfg, f, indent=2)

print("\nPatched fields now set to:")
pprint.pprint({k: cfg[k] for k in ("architectures", "score_dim", "score_type")})

## cd to you directory

import os
from huggingface_hub import login

# --------------------------------------------
# 1. Environment Setup
# --------------------------------------------

# Login to Hugging Face (required for downloading Beaver models)
HF_TOKEN = "hf_token"  # Replace with your token
login(HF_TOKEN)

# Set which GPUs to use (if you have)
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

# Redirect Hugging Face cache to shared project directory
HF_CACHE_DIR = "directory for cache to save space"
os.makedirs(HF_CACHE_DIR, exist_ok=True)
os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR
os.environ["HF_DATASETS_CACHE"] = HF_CACHE_DIR

# Optional: disable W&B logging
os.environ["WANDB_MODE"] = "disabled"

print("Hugging Face login & environment configured.")

# Pretrained actor and reward models (official Beaver-7B)
actor_model   = "PKU-Alignment/beaver-7b-v3.0"
reward_model  = "PKU-Alignment/beaver-7b-v3.0-reward"

# Your fine-tuned cost model
cost_model    = "path for the cost model"

# Output directory for PPO results
output_dir = "path to sotre new model"
os.makedirs(output_dir, exist_ok=True)

print("Model paths set.")
print(output_dir)

ppo_script_path = "path for .sh script to run the model for training"

# Use shell magic to run bash script from Python
!bash {ppo_script_path} \
    --actor_model_name_or_path {actor_model} \
    --reward_model_name_or_path {reward_model} \
    --cost_model_name_or_path {cost_model} \
    --cost_critic_model_name_or_path PKU-Alignment/beaver-7b-v3.0 \
    --output_dir {output_dir} \
    --offload optimizer \
    --zero_stage 3



